{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1743696608295,"user":{"displayName":"Miguel Monte e Freitas","userId":"18193102782690343097"},"user_tz":-60},"id":"EpsT7prUCO9h","outputId":"e9339cf9-5ff1-434e-982a-766c5a1655f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'SNLP' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/mig-mfreitas/SNLP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1957,"status":"ok","timestamp":1743697347679,"user":{"displayName":"Miguel Monte e Freitas","userId":"18193102782690343097"},"user_tz":-60},"id":"12I8xPPtDKRM","outputId":"3bd1c025-28b0-4d91-fdad-9a7a81c397b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## 1. API calls"],"metadata":{"id":"e1FoLS_eewTJ"}},{"cell_type":"markdown","source":["1. Install dependencies"],"metadata":{"id":"gxyUKbhDVuiB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KULEr_XwN-5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743089206797,"user_tz":-60,"elapsed":12751,"user":{"displayName":"Gonza","userId":"03064889314959799072"}},"outputId":"b7c7d413-0ee6-4c1f-86d8-73ebfa52e478","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.14)\n","Collecting azure-ai-inference\n","  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n","Collecting isodate>=0.6.1 (from azure-ai-inference)\n","  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n","Collecting azure-core>=1.30.0 (from azure-ai-inference)\n","  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (4.12.2)\n","Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.3)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.1.31)\n","Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: isodate, azure-core, azure-ai-inference\n","Successfully installed azure-ai-inference-1.0.0b9 azure-core-1.32.0 isodate-0.7.2\n"]}],"source":["# Install required packages\n","!pip install nest_asyncio tqdm aiohttp azure-ai-inference"]},{"cell_type":"markdown","source":["2. Link to data"],"metadata":{"id":"Y_gi25dzVRsU"}},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","folder_path = '/content/drive/MyDrive/dedicated_WAM/data/'\n","male_file_path = 'subsampled_BUG_50k_males.csv'\n","female_file_path = 'subsampled_BUG_50k_females.csv'\n","male_df = pd.read_csv(folder_path + male_file_path)\n","female_df = pd.read_csv(folder_path + female_file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAABP7F0wtYD","executionInfo":{"status":"ok","timestamp":1743089233097,"user_tz":-60,"elapsed":21998,"user":{"displayName":"Gonza","userId":"03064889314959799072"}},"outputId":"03b6915e-ee2f-497d-f606-bdb8bb675d8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Setup\n","endpoint = \"<>\"\n","api_key = \"<>\"\n","\n","# e.g. Mistral #\n","model = \"Llama-3.3-70B-Instruct\""],"metadata":{"id":"Hk5Cz4FA1IXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import asyncio\n","import time\n","import nest_asyncio\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from azure.ai.inference import ChatCompletionsClient\n","from azure.ai.inference.models import SystemMessage, UserMessage\n","from azure.core.credentials import AzureKeyCredential\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Apply nest_asyncio to allow asyncio in notebooks\n","nest_asyncio.apply()\n","\n","# Create the client\n","client = ChatCompletionsClient(\n","    endpoint=endpoint,\n","    credential=AzureKeyCredential(api_key),\n",")\n","\n","# Function to get completion for a single prompt\n","def get_completion(prompt_tuple, retry=0, max_retries=3):\n","    prompt_id, prompt_text = prompt_tuple\n","    try:\n","        response = client.complete(\n","            messages=[\n","                SystemMessage(content=\"You are a helpful assistant that continues the given text with one more sentence.\"),\n","                UserMessage(content=prompt_text)\n","            ],\n","            max_tokens=50,\n","            temperature=0.7,\n","            model=model\n","        )\n","        return (prompt_id, prompt_text, response.choices[0].message.content)\n","    except Exception as e:\n","        if retry < max_retries:\n","            # Exponential backoff\n","            time.sleep(1 * (2 ** retry))\n","            return get_completion(prompt_tuple, retry + 1, max_retries)\n","        return (prompt_id, prompt_text, f\"ERROR: {str(e)}\")\n","\n","# Process a batch of prompts using ThreadPoolExecutor\n","def process_batch(batch_prompts, max_workers=10):\n","    results = []\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        for result in executor.map(get_completion, batch_prompts):\n","            results.append(result)\n","    return results\n","\n","# Process all prompts in batches and save intermediate results\n","def process_all_prompts(input_df, output_folder, batch_size=20):\n","    # Create a list of (id, prompt) tuples\n","    prompt_tuples = list(zip(input_df['id'].tolist(), input_df['sentence_text'].tolist()))\n","\n","    all_completions = []\n","    total_batches = (len(prompt_tuples) + batch_size - 1) // batch_size\n","\n","    # Create progress bar\n","    progress_bar = tqdm(total=len(prompt_tuples), desc=\"Processing prompts\")\n","\n","    start_time = time.time()\n","\n","    for i in range(0, len(prompt_tuples), batch_size):\n","        batch = prompt_tuples[i:i + batch_size]\n","\n","        # Process this batch\n","        batch_results = process_batch(batch)\n","        all_completions.extend(batch_results)\n","\n","        # Update progress\n","        progress_bar.update(len(batch))\n","\n","        # Save intermediate results every 1000 prompts or at the end\n","        current_count = len(all_completions)\n","        if current_count % 1000 == 0 or current_count == len(prompt_tuples):\n","            # Create a DataFrame with all columns\n","            temp_df = pd.DataFrame(all_completions, columns=['id', 'prompt', 'completion'])\n","            output_path = os.path.join(output_folder, f'prompts_with_completions_{current_count}.csv')\n","            temp_df.to_csv(output_path, index=False)\n","\n","            # Calculate and display metrics\n","            elapsed = time.time() - start_time\n","            prompts_per_second = current_count / elapsed\n","            estimated_total = elapsed * (len(prompt_tuples) / current_count)\n","            remaining = estimated_total - elapsed\n","\n","            print(f\"Saved {current_count}/{len(prompt_tuples)} completions\")\n","            print(f\"Speed: {prompts_per_second:.2f} prompts/second\")\n","            print(f\"Time elapsed: {elapsed/60:.2f} minutes\")\n","            print(f\"Estimated time remaining: {remaining/60:.2f} minutes\")\n","\n","        # Small delay between batches to manage API load\n","        time.sleep(0.2)\n","\n","    progress_bar.close()\n","\n","    # Save final results\n","    final_df = pd.DataFrame(all_completions, columns=['id', 'prompt', 'completion'])\n","    final_df.to_csv('prompts_with_completions_final.csv', index=False)\n","\n","    print(f\"All done! Processed {len(prompt_tuples)} prompts in {(time.time() - start_time)/60:.2f} minutes\")\n","    return final_df\n","\n","# Test with a small subset first\n","def test_with_sample(input_df, sample_size=10):\n","    print(f\"Testing with {sample_size} prompts first...\")\n","    sample_df = input_df.head(sample_size)\n","    sample_tuples = list(zip(sample_df['id'].tolist(), sample_df['sentence_text'].tolist()))\n","\n","    for i, (prompt_id, prompt) in enumerate(sample_tuples):\n","        print(f\"\\nPrompt {i+1} (ID: {prompt_id}): {prompt}\")\n","        completion = get_completion((prompt_id, prompt))\n","        print(f\"Completion: {completion[2]}\")\n","        time.sleep(0.5)\n","\n","    print(\"\\nSample test completed successfully!\")\n","    return None\n"],"metadata":{"id":"2LkY__SC6t3K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test with a sample"],"metadata":{"id":"moi1AxM6o1HO"}},{"cell_type":"code","source":["test_with_sample(input_df, 3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4DrAK-nzxwx","executionInfo":{"status":"ok","timestamp":1743089423536,"user_tz":0,"elapsed":4951,"user":{"displayName":"Katherine Finch","userId":"00410021285208634876"}},"outputId":"e3324fe7-4e68-4e02-93ff-d62d0e44b937"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing with 3 prompts first...\n","\n","Prompt 1 (ID: 4): in her honour , the farmer named the transformed eggs with their delicate crystalline patterns on their surfaces \" pine - patterned eggs \" .\n","Completion: These exquisite \"pine-patterned eggs\" soon became a sought-after delicacy in the local market, with people traveling from afar to taste the unique and breathtakingly beautiful creations that bore the name of the farmer's beloved wife.\n","\n","Prompt 2 (ID: 9): the patient , her husband , and providers decided together that further management would be palliative .\n","Completion: This decision was followed by a comprehensive discussion about the patient's preferences and priorities for her remaining time, including symptom management, emotional support, and ongoing care at home.\n","\n","Prompt 3 (ID: 14): one housekeeper experienced itchiness of her face , angioedema , and lightheadedness immediately after inflating a latex balloon .\n","Completion: She was subsequently diagnosed with a latex allergy, which was likely triggered by the release of latex particles into the air as she inflated the balloon, prompting her employer to implement latex-free alternatives in the workplace.\n","\n","Sample test completed successfully!\n"]}]},{"cell_type":"markdown","source":["Run on full df"],"metadata":{"id":"GoR1XaRto6AR"}},{"cell_type":"code","source":["# Process all prompts with IDs preserved\n","output_folder = '/content/drive/MyDrive/dedicated_WAM/results/Completions/Mistral/25k-female/'\n","final_df = process_all_prompts(input_df, output_folder, batch_size=100)"],"metadata":{"id":"Vo0l4hHho_A3","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ce3bf240da42488fb7bb24e8e1140652","76a4ee4db0b94daa9d7f6e1ed3eec65d","716f5ddb95ff43c4af6195f2c059ca58","a6fd0af6badb402b995783808ab915dc","f24a682639d048dfb88b2c810d06c089","88a85f4bf98245239f590c77b7201f82","28fb268ff1d04d2db301a73883cfae76","08e87ddfec2e4bdc8ad2152e4de1c5be","69838eed54b441b5b6b17e74fa093d1b","70bfceabd217472f8acde6dc35fb8038","7571d58d5c6f48969663ccfa8dff9a80"]},"executionInfo":{"status":"ok","timestamp":1743088490350,"user_tz":0,"elapsed":2876344,"user":{"displayName":"Katherine Finch","userId":"00410021285208634876"}},"outputId":"e5c7e7ee-6e15-409a-d011-d35de11acf3f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Processing prompts:   0%|          | 0/25000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce3bf240da42488fb7bb24e8e1140652"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved 1000/25000 completions\n","Speed: 9.74 prompts/second\n","Time elapsed: 1.71 minutes\n","Estimated time remaining: 41.07 minutes\n","Saved 2000/25000 completions\n","Speed: 9.89 prompts/second\n","Time elapsed: 3.37 minutes\n","Estimated time remaining: 38.75 minutes\n","Saved 3000/25000 completions\n","Speed: 10.06 prompts/second\n","Time elapsed: 4.97 minutes\n","Estimated time remaining: 36.46 minutes\n","Saved 4000/25000 completions\n","Speed: 9.64 prompts/second\n","Time elapsed: 6.92 minutes\n","Estimated time remaining: 36.30 minutes\n","Saved 5000/25000 completions\n","Speed: 9.81 prompts/second\n","Time elapsed: 8.49 minutes\n","Estimated time remaining: 33.96 minutes\n","Saved 6000/25000 completions\n","Speed: 9.59 prompts/second\n","Time elapsed: 10.43 minutes\n","Estimated time remaining: 33.04 minutes\n","Saved 7000/25000 completions\n","Speed: 9.56 prompts/second\n","Time elapsed: 12.20 minutes\n","Estimated time remaining: 31.37 minutes\n","Saved 8000/25000 completions\n","Speed: 9.37 prompts/second\n","Time elapsed: 14.23 minutes\n","Estimated time remaining: 30.23 minutes\n","Saved 9000/25000 completions\n","Speed: 9.33 prompts/second\n","Time elapsed: 16.07 minutes\n","Estimated time remaining: 28.57 minutes\n","Saved 10000/25000 completions\n","Speed: 9.31 prompts/second\n","Time elapsed: 17.89 minutes\n","Estimated time remaining: 26.84 minutes\n","Saved 11000/25000 completions\n","Speed: 9.27 prompts/second\n","Time elapsed: 19.78 minutes\n","Estimated time remaining: 25.18 minutes\n","Saved 12000/25000 completions\n","Speed: 8.75 prompts/second\n","Time elapsed: 22.86 minutes\n","Estimated time remaining: 24.76 minutes\n","Saved 13000/25000 completions\n","Speed: 8.84 prompts/second\n","Time elapsed: 24.51 minutes\n","Estimated time remaining: 22.63 minutes\n","Saved 14000/25000 completions\n","Speed: 8.88 prompts/second\n","Time elapsed: 26.28 minutes\n","Estimated time remaining: 20.65 minutes\n","Saved 15000/25000 completions\n","Speed: 8.94 prompts/second\n","Time elapsed: 27.96 minutes\n","Estimated time remaining: 18.64 minutes\n","Saved 16000/25000 completions\n","Speed: 8.93 prompts/second\n","Time elapsed: 29.88 minutes\n","Estimated time remaining: 16.81 minutes\n","Saved 17000/25000 completions\n","Speed: 8.99 prompts/second\n","Time elapsed: 31.53 minutes\n","Estimated time remaining: 14.84 minutes\n","Saved 18000/25000 completions\n","Speed: 9.02 prompts/second\n","Time elapsed: 33.24 minutes\n","Estimated time remaining: 12.93 minutes\n","Saved 19000/25000 completions\n","Speed: 8.85 prompts/second\n","Time elapsed: 35.79 minutes\n","Estimated time remaining: 11.30 minutes\n","Saved 20000/25000 completions\n","Speed: 8.86 prompts/second\n","Time elapsed: 37.63 minutes\n","Estimated time remaining: 9.41 minutes\n","Saved 21000/25000 completions\n","Speed: 8.85 prompts/second\n","Time elapsed: 39.56 minutes\n","Estimated time remaining: 7.54 minutes\n","Saved 22000/25000 completions\n","Speed: 8.81 prompts/second\n","Time elapsed: 41.64 minutes\n","Estimated time remaining: 5.68 minutes\n","Saved 23000/25000 completions\n","Speed: 8.64 prompts/second\n","Time elapsed: 44.36 minutes\n","Estimated time remaining: 3.86 minutes\n","Saved 24000/25000 completions\n","Speed: 8.68 prompts/second\n","Time elapsed: 46.07 minutes\n","Estimated time remaining: 1.92 minutes\n","Saved 25000/25000 completions\n","Speed: 8.69 prompts/second\n","Time elapsed: 47.93 minutes\n","Estimated time remaining: 0.00 minutes\n","All done! Processed 25000 prompts in 47.94 minutes\n"]}]},{"cell_type":"code","source":["# Save the combined CSV\n","output_path = os.path.join('/content/drive/MyDrive/dedicated_WAM/results/Completions/Mistral/25k-female/', 'female_Mistral-Nemo.csv')\n","final_df.to_csv(output_path, index=False)\n","print(f\"Combined CSV saved to: {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yISeA7zDrWQf","executionInfo":{"status":"ok","timestamp":1743088490716,"user_tz":0,"elapsed":360,"user":{"displayName":"Katherine Finch","userId":"00410021285208634876"}},"outputId":"c6093ce1-83a7-456a-8a78-640ca9afee26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined CSV saved to: /content/drive/MyDrive/dedicated_WAM/results/Completions/Mistral/25k-female/female_Mistral-Nemo.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"zjDwTkRKXZlF"},"source":["## 2. Embed Completions to get feature activations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":18693,"status":"ok","timestamp":1743696511705,"user":{"displayName":"Miguel Monte e Freitas","userId":"18193102782690343097"},"user_tz":-60},"id":"vXrRcHbj_zNg","outputId":"f2a4e0e0-d3de-47d3-96a9-9060a96fed81"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) 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\n","Invalid input. Must be one of ('y', 'yes', '1', 'n', 'no', '0', '')\n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: fineGrained).\n","The token `dedicated wam` has been saved to /root/.cache/huggingface/stored_tokens\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `dedicated wam`\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8Xs34wMa9d7"},"outputs":[],"source":["input_df = pd.read_csv('/content/drive/MyDrive/dedicated_WAM/data/full_BUG_females.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUUELsZ1koa6"},"outputs":[],"source":["from SNLP.sae_encoding.utils import get_sae_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["d1576690cdb24f94bf9594fe2df42187","7d850b2654b24106bdddc7a028688da9","049e86003703433991b30adc315db7e1","d63c9d8e32f343208b1122ac583f19f6","49c32b3f33ae475e817bac5e6f3b7b84","aa30650a0fac4444956efa21a5adba12","7194498738854b3f9a5b5d5741b62ec6","7311ecadc86d43c297b3842f1523f330","86dd0df2b89c46729022dd4fd661e748","3a72bdac0eec4a18a03d3d4e5c884891","436f3f8cb12048a0b799f4ff61f9cf22"]},"executionInfo":{"elapsed":5999952,"status":"ok","timestamp":1743709721853,"user":{"displayName":"Miguel Monte e Freitas","userId":"18193102782690343097"},"user_tz":-60},"id":"lR7z-h9jeL4e","outputId":"c5ab8de7-2474-4826-8e55-de0d2e2fc5dd"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1576690cdb24f94bf9594fe2df42187"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gemma-2-2b into HookedTransformer\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9485/9485 [1:39:40<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Done!\n"]}],"source":["#input_df['completion'] = input_df['completion'].astype(str)\n","input_df['sentence_text'] = input_df['sentence_text'].astype(str)\n","input_list = input_df[\"sentence_text\"].tolist()\n","\n","# Get embeddings for each model's completions\n","sae_embeddings, seq_lengths = get_sae_embeddings(input_list,\n","                                                gemma_scope_sae_release=\"gemma-scope-2b-pt-res-canonical\",\n","                                                gemma_scope_sae_id=\"layer_25/width_16k/canonical\")\n","\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_n5Dr-3qj_B"},"outputs":[],"source":["from SNLP.sae_encoding.utils import save_npz\n","import numpy as np\n","\n","folder_path = '/content/drive/MyDrive/dedicated_WAM/results/Embeddings/Sparse_matrices/Prompts-Full'\n","\n","save_npz(f\"{folder_path}/female_sae_embeddings.npz\", sae_embeddings)\n","np.save(f\"{folder_path}/female_sae_embeddings_seq_lengths.npy\", np.array(seq_lengths, dtype=object))"]},{"cell_type":"markdown","metadata":{"id":"Z2_XOQLoSebm"},"source":["## 3. Combine sparse matrices per prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i21AztHMxDmu"},"outputs":[],"source":["from SNLP.sae_encoding.utils import load_npz\n","\n","def load_sparse(folder_path, file_name):\n","    big_sparse = load_npz(f\"{folder_path}{file_name}.npz\")\n","    seq_lengths = np.load(f\"{folder_path}{file_name}_seq_lengths.npy\", allow_pickle=True).tolist()\n","\n","    result = []\n","    idx = 0\n","    for L in seq_lengths:\n","        prompt_feats = big_sparse[idx: idx + L, :]\n","        result.append(prompt_feats)\n","        idx += L\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEk-hl9iDMXE"},"outputs":[],"source":["import numpy as np\n","\n","from re import S\n","folder_path = \"/content/drive/MyDrive/dedicated_WAM/results/Embeddings/Sparse_matrices/Llama-25k\"\n","\n","female_prompts = \"/female_prompts_sae_embeddings\"\n","male_prompts = \"/male_prompts_sae_embeddings\"\n","female_outputs = \"/female_sae_embeddings\"\n","male_outputs = \"/male_sae_embeddings\"\n","\n","she_act = load_sparse(folder_path, female_outputs)\n","he_act = load_sparse(folder_path, male_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_ojIHQXEVZ8"},"outputs":[],"source":["# Compute max activation per sentence\n","he_max = np.array([sparse.max(axis=0).toarray().flatten() for sparse in he_act])\n","she_max = np.array([sparse.max(axis=0).toarray().flatten() for sparse in she_act])\n","\n","# Compute sum of activations per sentence\n","he_sum = np.array([np.asarray(sparse.sum(axis=0)).flatten() for sparse in he_act])\n","she_sum = np.array([np.asarray(sparse.sum(axis=0)).flatten() for sparse in she_act])\n","\n","# Compute mean of activations per sentence\n","he_mean = np.array([np.asarray(sparse.mean(axis=0)).flatten() for sparse in he_act])\n","she_mean = np.array([np.asarray(sparse.mean(axis=0)).flatten() for sparse in she_act])\n","\n","# Compute mean activation per feature\n","he_mean_global = he_max.mean(axis=0)\n","she_mean_global = she_max.mean(axis=0)\n","\n","# Compute max activation seen per feature\n","he_feature_max = he_max.max(axis=0)  # Max seen across all sentences\n","she_feature_max = she_max.max(axis=0)\n","feature_max = np.maximum(he_feature_max, she_feature_max)\n","\n","# Normalize each feature to a score between 0 and 1\n","he_score = he_mean_global / (feature_max + 1e-10)\n","she_score = she_mean_global / (feature_max + 1e-10)\n","score_diff = np.abs(he_score - she_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54508,"status":"ok","timestamp":1743495065219,"user":{"displayName":"Katherine Finch","userId":"00410021285208634876"},"user_tz":-60},"id":"eWQWZvsRFwb_","outputId":"031108b0-8b36-4047-d975-c99a5cf2db27"},"outputs":[{"name":"stdout","output_type":"stream","text":["All outputs saved to /content/drive/MyDrive/dedicated_WAM/results/Embeddings/Llama-25k/\n"]}],"source":["# Define the output directory\n","output_dir = \"/content/drive/MyDrive/dedicated_WAM/results/Embeddings/Llama-25k/\"\n","\n","# Save the arrays using NumPy's save function\n","np.save(output_dir + \"he_max.npy\", he_max)\n","np.save(output_dir + \"she_max.npy\", she_max)\n","np.save(output_dir + \"he_sum.npy\", he_sum)\n","np.save(output_dir + \"she_sum.npy\", she_sum)\n","np.save(output_dir + \"he_mean.npy\", he_mean)\n","np.save(output_dir + \"she_mean.npy\", she_mean)\n","np.save(output_dir + \"he_mean_global.npy\", he_mean_global)\n","np.save(output_dir + \"she_mean_global.npy\", she_mean_global)\n","np.save(output_dir + \"he_feature_max.npy\", he_feature_max)\n","np.save(output_dir + \"she_feature_max.npy\", she_feature_max)\n","np.save(output_dir + \"feature_max.npy\", feature_max)\n","np.save(output_dir + \"he_score.npy\", he_score)\n","np.save(output_dir + \"she_score.npy\", she_score)\n","np.save(output_dir + \"score_diff.npy\", score_diff)\n","\n","print(\"All outputs saved to\", output_dir)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"14zIPGVY7gI5A227gE6KMUkFroeqJVmAL","timestamp":1744735533627}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d1576690cdb24f94bf9594fe2df42187":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d850b2654b24106bdddc7a028688da9","IPY_MODEL_049e86003703433991b30adc315db7e1","IPY_MODEL_d63c9d8e32f343208b1122ac583f19f6"],"layout":"IPY_MODEL_49c32b3f33ae475e817bac5e6f3b7b84"}},"7d850b2654b24106bdddc7a028688da9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa30650a0fac4444956efa21a5adba12","placeholder":"​","style":"IPY_MODEL_7194498738854b3f9a5b5d5741b62ec6","value":"Loading checkpoint shards: 100%"}},"049e86003703433991b30adc315db7e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7311ecadc86d43c297b3842f1523f330","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86dd0df2b89c46729022dd4fd661e748","value":3}},"d63c9d8e32f343208b1122ac583f19f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a72bdac0eec4a18a03d3d4e5c884891","placeholder":"​","style":"IPY_MODEL_436f3f8cb12048a0b799f4ff61f9cf22","value":" 3/3 [00:01&lt;00:00,  1.28it/s]"}},"49c32b3f33ae475e817bac5e6f3b7b84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa30650a0fac4444956efa21a5adba12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7194498738854b3f9a5b5d5741b62ec6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7311ecadc86d43c297b3842f1523f330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86dd0df2b89c46729022dd4fd661e748":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a72bdac0eec4a18a03d3d4e5c884891":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"436f3f8cb12048a0b799f4ff61f9cf22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}